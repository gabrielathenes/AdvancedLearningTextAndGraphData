{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copie de Altegrad_2021_Lab2Sergio.ipynb","provenance":[{"file_id":"1BYpuIN6RjmX3a-_0t7mKEYHMNhXyV9mN","timestamp":1637696694075}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"lCJvlnvsKALE"},"source":["<center><h2>ALTeGraD 2021<br>Lab Session 2: NMT</h2><h3> Neural Machine Translation</h3> 16 / 11 / 2021<br> M. Kamal Eddine, H. Abdine</center>"]},{"cell_type":"code","metadata":{"id":"DB6pvLvlKbtD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637696617203,"user_tz":-60,"elapsed":726,"user":{"displayName":"gabriel athenes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14991399304941578798"}},"outputId":"cb9082d2-9205-4eda-bf22-dd3a628e1347"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils import data\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm import tqdm\n","from nltk import word_tokenize\n","import nltk\n","nltk.download (\"punkt\")"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"wqIFlSfYTwk8"},"source":["## Define the Encoder / Task 1"]},{"cell_type":"code","metadata":{"id":"Kc8cQTFkKmif","executionInfo":{"status":"ok","timestamp":1637696620065,"user_tz":-60,"elapsed":289,"user":{"displayName":"gabriel athenes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14991399304941578798"}}},"source":["class Encoder(nn.Module):\n","    '''\n","    to be passed the entire source sequence at once\n","    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n","    https://pytorch.org/docs/stable/nn.html#gru\n","    '''\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n","        super(Encoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n","        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n","    \n","    def forward(self, input):\n","        # fill the gaps # (transform input into embeddings and pass embeddings to RNN)\n","        # you should return a tensor of shape (seq, batch, feat)\n","        embedding=self.embedding(input)\n","        output,h=self.rnn(embedding)\n","        return output"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bn9iO9wNT2p7"},"source":["## Define the Attention layer / Task 3"]},{"cell_type":"code","metadata":{"id":"JwUAUDL4KmoM","executionInfo":{"status":"ok","timestamp":1637696622828,"user_tz":-60,"elapsed":460,"user":{"displayName":"gabriel athenes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14991399304941578798"}}},"source":["class seq2seqAtt(nn.Module):\n","    '''\n","    concat global attention a la Luong et al. 2015 (subsection 3.1)\n","    https://arxiv.org/pdf/1508.04025.pdf\n","    '''\n","    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n","        super(seq2seqAtt, self).__init__()\n","        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t, hidden_dim)\n","        self.ff_score = nn.Linear(hidden_dim, 1, bias=False) # just a dot product here\n","    \n","    def forward(self, target_h, source_hs):\n","        target_h_rep = target_h.repeat(source_hs.size(0), 1, 1) # (1, batch, feat) -> (seq, batch, feat)\n","        # fill the gaps #\n","        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n","        concat_output = self.ff_concat(torch.cat((target_h_rep, source_hs), 2))\n","        scores = self.ff_score(torch.tanh(concat_output))# should be of shape (seq, batch, 1)\n","        scores = scores.squeeze(dim=2) # (seq, batch, 1) -> (seq, batch). dim = 2 because we don't want to squeeze the batch dim if batch size = 1\n","        norm_scores = torch.softmax(scores, 0)\n","        source_hs_p = source_hs.permute((2, 0, 1)) # (seq, batch, feat) -> (feat, seq, batch)\n","        weighted_source_hs = (norm_scores * source_hs_p) # (seq, batch) * (feat, seq, batch) (* checks from right to left that the dimensions match)\n","        ct = torch.sum(weighted_source_hs.permute((1, 2, 0)), 0, keepdim=True) # (feat, seq, batch) -> (seq, batch, feat) -> (1, batch, feat); keepdim otherwise sum squeezes \n","        return ct, norm_scores"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FNnGEa5cT9ka"},"source":["## Define the Decoder layer / Task 2"]},{"cell_type":"code","metadata":{"id":"h7tLaq4PK90q","executionInfo":{"status":"ok","timestamp":1637696625605,"user_tz":-60,"elapsed":310,"user":{"displayName":"gabriel athenes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14991399304941578798"}}},"source":["class Decoder(nn.Module):\n","    '''to be used one timestep at a time\n","       see https://pytorch.org/docs/stable/nn.html#gru'''\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n","        super(Decoder, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n","        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n","        self.ff_concat = nn.Linear(2*hidden_dim, hidden_dim)\n","        self.predict = nn.Linear(hidden_dim, vocab_size)\n","    \n","    def forward(self, input, source_context, h):\n","        # fill the gaps #\n","        # transform input into embeddings, pass embeddings to RNN, concatenate with source_context and apply tanh, and make the prediction\n","        # prediction should be of shape (1, batch, vocab), h and tilde_h of shape (1, batch, feat)\n","        embedding = self.embedding(input)\n","        h = self.rnn(embedding,h)[1]\n","        concat_out = self.ff_concat(torch.cat((source_context, h), 2))\n","        tilde_h = torch.tanh(concat_out)\n","        prediction = self.predict(tilde_h)\n","        return prediction, h"],"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VUT6D3JETX8H"},"source":["# Define the full seq2seq model / Task 4:"]},{"cell_type":"code","metadata":{"id":"FYX0K3dNK-c9","executionInfo":{"status":"ok","timestamp":1637696627853,"user_tz":-60,"elapsed":475,"user":{"displayName":"gabriel athenes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14991399304941578798"}}},"source":["class seq2seqModel(nn.Module):\n","    '''the full seq2seq model'''\n","    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n","     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n","     'oov_token','sos_token','eos_token','max_size']\n","    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t, \n","                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n","                 oov_token, sos_token, eos_token, max_size):\n","        super(seq2seqModel, self).__init__()\n","        self.vocab_s = vocab_s\n","        self.source_language = source_language\n","        self.vocab_t_inv = vocab_t_inv\n","        self.embedding_dim_s = embedding_dim_s\n","        self.embedding_dim_t = embedding_dim_t\n","        self.hidden_dim_s = hidden_dim_s\n","        self.hidden_dim_t = hidden_dim_t\n","        self.hidden_dim_att = hidden_dim_att\n","        self.do_att = do_att # should attention be used?\n","        self.padding_token = padding_token\n","        self.oov_token = oov_token\n","        self.sos_token = sos_token\n","        self.eos_token = eos_token\n","        self.max_size = max_size\n","        \n","        self.max_source_idx = max(list(vocab_s.values()))\n","        print('max source index',self.max_source_idx)\n","        print('source vocab size',len(vocab_s))\n","        \n","        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n","        print('max target index',self.max_target_idx)\n","        print('target vocab size',len(vocab_t_inv))\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.encoder = Encoder(self.max_source_idx+1, self.embedding_dim_s, self.hidden_dim_s, self.padding_token).to(self.device)\n","        self.decoder = Decoder(self.max_target_idx+1, self.embedding_dim_t, self.hidden_dim_t, self.padding_token).to(self.device)\n","        \n","        if self.do_att:\n","            self.att_mech = seq2seqAtt(self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t).to(self.device)\n","    \n","    def my_pad(self, my_list):\n","        '''my_list is a list of tuples of the form [(tensor_s_1, tensor_t_1), ..., (tensor_s_batch, tensor_t_batch)]\n","        the <eos> token is appended to each sequence before padding\n","        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n","        batch_source = pad_sequence([torch.cat((elt[0], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n","        batch_target = pad_sequence([torch.cat((elt[1], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n","        return batch_source, batch_target\n","    \n","    def forward(self, input, max_size, is_prod):\n","        if is_prod: \n","            input = input.unsqueeze(1) # (seq) -> (seq, 1) 1D input <=> we receive just one sentence as input (predict/production mode)\n","        current_batch_size = input.size(1)\n","        # fill the gap #\n","        # use the encoder\n","        source_hs = self.encoder(input)\n","        # = = = decoder part (one timestep at a time)  = = =\n","        target_h = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(self.device) # init (1, batch, feat)\n","        \n","        # fill the gap #\n","        # (initialize target_input with the proper token)\n","        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1, batch)\n","        pos = 0\n","        eos_counter = 0\n","        logits = []\n","        aligns = []\n","        while True:\n","            if self.do_att:\n","                source_context, align = self.att_mech(target_h, source_hs) # (1, batch, feat)\n","                aligns.append(align)\n","            else:\n","                source_context = source_hs[-1, :, :].unsqueeze(0) # (1, batch, feat) last hidden state of encoder\n","            # fill the gap #\n","            # use the decoder\n","            prediction, target_h = self.decoder(target_input, source_context, target_h)\n","            logits.append(prediction) # (1, batch, vocab)\n","            # fill the gap #\n","            # get the next input to pass the decoder\n","            target_input =  torch.argmax(torch.softmax(prediction, axis=-1), axis=-1)\n","            eos_counter += torch.sum(target_input==self.eos_token).item()\n","            pos += 1\n","            if pos >= max_size or (eos_counter == current_batch_size and is_prod):\n","                break\n","        to_return = torch.cat(logits, 0) # logits is a list of tensors -> (seq, batch, vocab)\n","        \n","        if is_prod:\n","            to_return = to_return.squeeze(dim=1) # (seq, vocab)\n","        \n","        return to_return,aligns\n","    \n","    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n","        parameters = [p for p in self.parameters() if p.requires_grad]\n","        optimizer = optim.Adam(parameters, lr=lr)\n","        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n","        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n","        # we pass a collate function to perform padding on the fly, within each batch\n","        # this is better than truncation/padding at the dataset level\n","        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size, \n","                                       shuffle=True, collate_fn=self.my_pad) # returns (batch, seq)\n","        test_loader = data.DataLoader(testDataset, batch_size=512,\n","                                      collate_fn=self.my_pad)\n","        tdqm_dict_keys = ['loss', 'test loss']\n","        tdqm_dict = dict(zip(tdqm_dict_keys, [0.0, 0.0]))\n","        patience_counter = 1\n","        patience_loss = 99999\n","        \n","        for epoch in range(n_epochs): \n","            with tqdm(total=len(train_loader), unit_scale=True, postfix={'loss':0.0, 'test loss':0.0},\n","                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1), ncols=100) as pbar:\n","                for loader_idx, loader in enumerate([train_loader, test_loader]):\n","                    total_loss = 0\n","                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","                    if loader_idx == 0:\n","                        self.train()\n","                    else:\n","                        self.eval()\n","                    for i, (batch_source, batch_target) in enumerate(loader):\n","                        batch_source = batch_source.transpose(1, 0).to(self.device) # RNN needs (seq, batch, feat) but loader returns (batch, seq)                        \n","                        batch_target = batch_target.transpose(1, 0).to(self.device) # (seq, batch)\n","                        \n","                        # are we using the model in production\n","                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq, batch), i.e., train or test\n","                        if is_prod:\n","                            max_size = self.max_size\n","                            self.eval()\n","                        else:\n","                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n","                        \n","                        unnormalized_logits = self.forward(batch_source, max_size, is_prod)[0]\n","                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1), batch_target.flatten())\n","                        total_loss += sentence_loss.item()                        \n","                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)                       \n","                        pbar.set_postfix(tdqm_dict)                     \n","                        if loader_idx == 0:\n","                            optimizer.zero_grad() # flush gradient attributes\n","                            sentence_loss.backward() # compute gradients\n","                            optimizer.step() # update\n","                            pbar.update(1)\n","            \n","            if total_loss > patience_loss:\n","                patience_counter += 1\n","            else:\n","                patience_loss = total_loss\n","                patience_counter = 1 # reset\n","            \n","            if patience_counter > patience:\n","                break\n","    \n","    def sourceNl_to_ints(self, source_nl):\n","        '''converts natural language source sentence into source integers'''\n","        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n","        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n","        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n","                       self.oov_token for elt in source_nl_clean_tok]\n","        \n","        source_ints = torch.LongTensor(source_ints).to(self.device)\n","        return source_ints \n","    \n","    def targetInts_to_nl(self, target_ints):\n","        '''converts integer target sentence into target natural language'''\n","        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n","                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n","                else self.vocab_t_inv[elt] for elt in target_ints]\n","    \n","    def predict(self, source_nl):\n","        source_ints = self.sourceNl_to_ints(source_nl)\n","        logits = self.forward(source_ints, self.max_size, True)[0] # (seq) -> (<=max_size, vocab)\n","        target_ints = logits.argmax(-1).squeeze() # (<=max_size, 1) -> (<=max_size)\n","        target_nl = self.targetInts_to_nl(target_ints.tolist())\n","        return ' '.join(target_nl)\n","    def predict_score(self,source_nl):\n","        source_ints = self.sourceNl_to_ints(source_nl)\n","        return self.forward(source_ints,self.max_size,True)[1]\n","    def save(self, path_to_file):\n","        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n","        attrs['state_dict'] = self.state_dict()\n","        torch.save(attrs, path_to_file)\n","    \n","    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n","    def load(cls, path_to_file):\n","        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n","        state_dict = attrs.pop('state_dict')\n","        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n","        new.load_state_dict(state_dict)\n","        return new        "],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"B5RprtnBK-ia","executionInfo":{"status":"ok","timestamp":1637696635150,"user_tz":-60,"elapsed":337,"user":{"displayName":"gabriel athenes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14991399304941578798"}}},"source":["import sys\n","import json\n","\n","import torch\n","from torch.utils import data"],"execution_count":42,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PgkVw6lVUIT3"},"source":["## Prepare the Data:"]},{"cell_type":"code","metadata":{"id":"datl5SFtJ9Br","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637696653658,"user_tz":-60,"elapsed":17200,"user":{"displayName":"gabriel athenes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14991399304941578798"}},"outputId":"efb7f3c5-df6a-4181-8590-aabb2be8a656"},"source":["!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199291&authkey=AMIEuRcvDQWgoZo\" -O \"data.zip\"\n","!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199292&authkey=ANLtZTfpmk6tcE0\" -O \"pretrained_moodle.pt\"\n","!unzip data.zip\n","\n","path_to_data = './'\n","path_to_save_models = './'"],"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-11-23 19:43:56--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199291&authkey=AMIEuRcvDQWgoZo\n","Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n","Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://vgtdqw.am.files.1drv.com/y4mZ6rGRxaUlTyIAKwGhc-y4GMgQDkYBb8t3iivF4lSJKe6-vW_GYcTNb2Uz_2L-V5o7RMmoMnaos_OioTZndoBkvbj9UYI32bzGL9TxsZOkr6jKG2b2qYNGN-ZQt4HYqX9C775kEaaJT2HgQBf0xoeGTrHIdhpT-xK3k06aEB35tpsUAlqL8kr-ns94i_0zrxTBuZjN0-Zg63KU70A113C5w/data.zip?download&psid=1 [following]\n","--2021-11-23 19:44:02--  https://vgtdqw.am.files.1drv.com/y4mZ6rGRxaUlTyIAKwGhc-y4GMgQDkYBb8t3iivF4lSJKe6-vW_GYcTNb2Uz_2L-V5o7RMmoMnaos_OioTZndoBkvbj9UYI32bzGL9TxsZOkr6jKG2b2qYNGN-ZQt4HYqX9C775kEaaJT2HgQBf0xoeGTrHIdhpT-xK3k06aEB35tpsUAlqL8kr-ns94i_0zrxTBuZjN0-Zg63KU70A113C5w/data.zip?download&psid=1\n","Resolving vgtdqw.am.files.1drv.com (vgtdqw.am.files.1drv.com)... 13.107.42.12\n","Connecting to vgtdqw.am.files.1drv.com (vgtdqw.am.files.1drv.com)|13.107.42.12|:443... connected.\n","HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n","\n","    The file is already fully retrieved; nothing to do.\n","\n","--2021-11-23 19:44:03--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199292&authkey=ANLtZTfpmk6tcE0\n","Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n","Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://vgtcqw.am.files.1drv.com/y4mAjBpveOIIyISTwbsqK_PALq4GbGSS7KtjwjnNuzmK4Aqkn7b9shvbje6eN_liRQ_nvzfgr0iD6IrnC63Andf8WYhsY7g5-4gastqUOj0DBJeKCFozu8wUVNZrFuJfECcbLcHZ1zAmKlJnMa2-_RBaRA5BS3SWEnvt1nMVRUXS_ZAFP_oemNG3bvM9OsZ1XN2u0rQtqAHy5trJc2GuP_s4g/pretrained_moodle.pt?download&psid=1 [following]\n","--2021-11-23 19:44:04--  https://vgtcqw.am.files.1drv.com/y4mAjBpveOIIyISTwbsqK_PALq4GbGSS7KtjwjnNuzmK4Aqkn7b9shvbje6eN_liRQ_nvzfgr0iD6IrnC63Andf8WYhsY7g5-4gastqUOj0DBJeKCFozu8wUVNZrFuJfECcbLcHZ1zAmKlJnMa2-_RBaRA5BS3SWEnvt1nMVRUXS_ZAFP_oemNG3bvM9OsZ1XN2u0rQtqAHy5trJc2GuP_s4g/pretrained_moodle.pt?download&psid=1\n","Resolving vgtcqw.am.files.1drv.com (vgtcqw.am.files.1drv.com)... 13.107.42.12\n","Connecting to vgtcqw.am.files.1drv.com (vgtcqw.am.files.1drv.com)|13.107.42.12|:443... connected.\n","HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n","\n","    The file is already fully retrieved; nothing to do.\n","\n","Archive:  data.zip\n","replace pairs_test_ints.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"]}]},{"cell_type":"code","metadata":{"id":"wZCiFl61LPQj","executionInfo":{"status":"ok","timestamp":1637696655930,"user_tz":-60,"elapsed":235,"user":{"displayName":"gabriel athenes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14991399304941578798"}}},"source":["class Dataset(data.Dataset):\n","  def __init__(self, pairs):\n","        self.pairs = pairs\n","\n","  def __len__(self):\n","        return len(self.pairs) # total nb of observations\n","\n","  def __getitem__(self, idx):\n","        source, target = self.pairs[idx] # one observation\n","        return torch.LongTensor(source), torch.LongTensor(target)\n","\n","def load_pairs(train_or_test):\n","    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n","        pairs_tmp = file.read().splitlines()\n","    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n","    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n","                  elt[1].split()]] for elt in pairs_tmp]\n","    return pairs_tmp"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"tCsAk4ILTkEc"},"source":["## Training / Task 5:"]},{"cell_type":"code","metadata":{"hidden":true,"id":"kSZ-cvSuLQVt","colab":{"base_uri":"https://localhost:8080/","height":502},"executionInfo":{"status":"error","timestamp":1637696677720,"user_tz":-60,"elapsed":19634,"user":{"displayName":"gabriel athenes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14991399304941578798"}},"outputId":"0f304bef-d04a-4bda-bed8-440aeacbc309"},"source":["do_att = True # should always be set to True\n","is_prod = False # production mode or not\n","\n","if not is_prod:\n","        \n","    pairs_train = load_pairs('train')\n","    pairs_test = load_pairs('test')\n","    \n","    with open(path_to_data + 'vocab_source.json','r') as file:\n","        vocab_source = json.load(file) # word -> index\n","    \n","    with open(path_to_data + 'vocab_target.json','r') as file:\n","        vocab_target = json.load(file) # word -> index\n","    \n","    vocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\n","    \n","    print('data loaded')\n","        \n","    training_set = Dataset(pairs_train)\n","    test_set = Dataset(pairs_test)\n","    \n","    print('data prepared')\n","    \n","    print('= = = attention-based model?:',str(do_att),'= = =')\n","    \n","    model = seq2seqModel(vocab_s=vocab_source,\n","                         source_language='english',\n","                         vocab_t_inv=vocab_target_inv,\n","                         embedding_dim_s=40,\n","                         embedding_dim_t=40,\n","                         hidden_dim_s=30,\n","                         hidden_dim_t=30,\n","                         hidden_dim_att=20,\n","                         do_att=do_att,\n","                         padding_token=0,\n","                         oov_token=1,\n","                         sos_token=2,\n","                         eos_token=3,\n","                         max_size=30) # max size of generated sentence in prediction mode\n","    \n","    model.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=20,patience=2)\n","    model.save(path_to_save_models + 'my_model.pt')"],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["data loaded\n","data prepared\n","= = = attention-based model?: True = = =\n","max source index 5281\n","source vocab size 5278\n","max target index 7459\n","target vocab size 7456\n"]},{"output_type":"stream","name":"stderr","text":["Epoch : 0/19:   3%|▋                    | 74.0/2.13k [00:16<07:52, 4.36it/s, loss=7.75, test loss=0]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-6f9f8ffee17b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                          max_size=30) # max size of generated sentence in prediction mode\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_save_models\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'my_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-41-b86a6c4ec86c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0munnormalized_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_prod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                         \u001b[0msentence_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munnormalized_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msentence_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0mtdqm_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtdqm_dict_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloader_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"pf0rN4RPToom"},"source":["## Testing / Task 6:"]},{"cell_type":"code","metadata":{"id":"VhXbQjP_YrgY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637696684887,"user_tz":-60,"elapsed":795,"user":{"displayName":"gabriel athenes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14991399304941578798"}},"outputId":"bf008ec7-52f1-4995-91c1-ea5a8ec202f4"},"source":["is_prod = True # production mode or not\n","\n","if is_prod:\n","    model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\n","    \n","    to_test = ['I am a student.',\n","               'I have a red car.',  # inversion captured\n","               'I love playing video games.',\n","                'This river is full of fish.', # plein vs pleine (accord)\n","                'The fridge is full of food.',\n","               'The cat fell asleep on the mat.',\n","               'my brother likes pizza.', # pizza is translated to 'la pizza'\n","               'I did not mean to hurt you.', # translation of mean in context\n","               'She is so mean.',\n","               'Help me pick out a tie to go with this suit!', # right translation\n","               \"I can't help but smoking weed.\", # this one and below: hallucination\n","               'The kids were playing hide and seek.',\n","               'The cat fell asleep in front of the fireplace.']\n","    \n","    for elt in to_test:\n","        print('= = = = = \\n','%s -> %s' % (elt, model.predict(elt)))"],"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["max source index 5281\n","source vocab size 5278\n","max target index 7459\n","target vocab size 7456\n","= = = = = \n"," I am a student. -> je suis étudiant . . . . . . . . . . . . . . . . . . . . . . . . . . .\n","= = = = = \n"," I have a red car. -> j ai une voiture rouge . . . . . . . . . . . . . . . . . . . . . . . . .\n","= = = = = \n"," I love playing video games. -> j adore jouer à jeux jeux jeux vidéo . . . . . . . . . . . . . . . . . . . . . .\n","= = = = = \n"," This river is full of fish. -> cette rivière est pleine de poisson . . . . . . . . . . . . . . . . . . . . . . . .\n","= = = = = \n"," The fridge is full of food. -> le frigo est plein de nourriture . . . . . . . . . . . . . . . . . . . . . . . .\n","= = = = = \n"," The cat fell asleep on the mat. -> le chat s est endormi sur le tapis . . . . . . . . . . . . . . . . . . . . . .\n","= = = = = \n"," my brother likes pizza. -> mon frère aime la pizza . . . . . . . . . . . . . . . . . . . . . . . . .\n","= = = = = \n"," I did not mean to hurt you. -> je n ai pas voulu intention de te blesser . . . . . . . . . . . . . . . . . . . . .\n","= = = = = \n"," She is so mean. -> elle est tellement méchant . <EOS>\n","= = = = = \n"," Help me pick out a tie to go with this suit! -> aidez moi à chercher une cravate pour aller avec ceci ! ! ! ! ! ! ! ! ! ! ! ! ! ! <EOS>\n","= = = = = \n"," I can't help but smoking weed. -> je ne peux pas empêcher de de fumer fumer . . . . . . . . . . . . . . . . . . . . .\n","= = = = = \n"," The kids were playing hide and seek. -> les enfants jouent cache cache cache . . . . . . . . . . . . . . . . . . . . . . . .\n","= = = = = \n"," The cat fell asleep in front of the fireplace. -> le chat s est en du la . . . . . . . . . . . . . . . . . . . . . . .\n"]}]},{"cell_type":"code","metadata":{"id":"mRCGuGzZDH6h","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1637692305603,"user_tz":-60,"elapsed":294,"user":{"displayName":"gabriel athenes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14991399304941578798"}},"outputId":"f7a4b543-2aba-4da1-f89c-d3a837e76917"},"source":["sentence = 'Video games are for kids.'\n","translation = model.predict(sentence)\n","scores=model.predict_score(sentence)\n","translation"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'la jeux vidéo sont pour les enfants . . . . . . . . . . . . . . . . . . . . . . .'"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"6reuQr_FDH6h","executionInfo":{"status":"ok","timestamp":1637692340540,"user_tz":-60,"elapsed":363,"user":{"displayName":"gabriel athenes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14991399304941578798"}},"outputId":"1ce6dace-c4b3-413d-cb6f-1f95fa1d105c"},"source":["import matplotlib.pyplot as plt\n","y = sentence.split(' ')\n","y[-1] = yticks[-1][:-1]\n","y += ['.']\n","x = translation.split(' ')[:9]\n","fig, ax = plt.subplots(1,1)\n","im = ax.imshow(torch.cat(scores[:8], 1).detach().numpy())\n","ax.set_yticks(range(6))\n","ax.set_yticklabels(yticks)\n","ax.set_xticks(range(8))\n","ax.set_xticklabels(xticks,rotation=40)\n","fig.colorbar(im)"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.colorbar.Colorbar at 0x7f0e938ff690>"]},"metadata":{},"execution_count":36},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXUAAAEGCAYAAACaSwWnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfzUlEQVR4nO3de5xdZXn28d+VgXDGAwEFEiQoVpGDSowgoBRQAyp4agVBpVpSbfFEqy9aRaTWV1FRrEiNSEFtReuJCFFAxNeqgAkgIFAqgkqAgiAgZ5KZ6/3jWUO2YzKzd7L3XouV6/v5rM/svfaate6Z2XOvZ9/Ps54l20RERDtMqzuAiIjonyT1iIgWSVKPiGiRJPWIiBZJUo+IaJEk9YiIFlmn7gAiIh4tXvznG/mO3492te0lVzx0ju15Aw7pTySpR0R06fbfj3LxOTO72nbdLX81Y8DhrFSSekRE18yox+oOYlJJ6hERXTIwRrOvwk9Sj4jokjHL3F1NvS5J6hERPUhLPSKiJQyMJqlHRLRHWuoDNrLpRl5388fWGsP06x+o9fgR0Z17uPN225uv7vcbGG34dOWP+qS+7uaP5UnH/02tMWzzmqtrPf4jxprdgRNRt+/7679Z0300e0BjC5J6RMSw2ObhtNQjItqhjFNvtiT1iIiuiVFUdxCTSlKPiOiSgbFmV1+S1CMiepGWekRES5SLj5LUIyJawcAyN/veQknqERFdMmK04TeMS1KPiOjBmFN+iYhohdTUIyJaRYw2vKa+WtFJukDSiyese4ekGyQdvYrvuXd1jhUR0RQGljHS1VKX1W2pfwU4GDinY93BwBts/2iNo4qIaCC7pS114OvASyRNB5C0LbAV8GRJn6nWzZZ0oaQrJX2o85slvUvSYklXSPpgx/qjJP2iWt6xmrFFRAzMGOpqqctqJXXbvwd+BuxfrToY+Br80ezxJwIn294JuGV8paQXAdsDc4FnArtKer6kXYG/Ap4L7AYcIelZqxNfRMQglI7SaV0tdVmTI4+XYKi+fmXC63t0rPtSx/oXVctlwKXA0yhJfk/gW7bvs30v8E1gr5UdWNJ8SUskLRn9w31r8CNERPSilF+6WeqyJkc+E9hX0rOBDW1fspJtVjb1jYD/a/uZ1fIU21/o5cC2F9ieY3vOyKYbrUboERG9K1PvTutqqctqH7lqTV8AnMqfttIBfsKKlvyhHevPAd4oaWMASVtL2gL4L+DlkjaUtBHwimpdREQjGPGwR7pa6rKm49S/AnyLFcm709uB/5D0fyitegBsnyvp6cCFkgDuBQ6zfamk0yi1eoBTbF+2hvFFRPTVWMNHv6xRUrf9bVjRzWv7NOC06vENwO4dm7+vY7sTKR2pE/d3AnDCmsQUETEo4x2lTZYrSiMiumTEaMPnfmn2KSciomH62VEqaZ6kayVdt7Kr8SVtU13Bf1l1Xc8BU+0zLfWIiC7ZYlmfOkEljQAnAS8ElgKLJS20fXXHZu8Dvmb7ZEk7AIuAbSfbb5J6RESXDP0cgz4XuM729QCSzgAOAjqTuoFNq8ePAW6eaqdJ6hERPehjR+nWwI0dz5dSrqjvdCxwrqS3AhsB+02109TUIyK6ZMSYu1uAGeNXvlfL/NU45CHAabZnAgcAX5I0ad5OSz0iogc9tNRvtz1nktdvAmZ1PJ9Zrev0JmAegO0LJa0PzABuW9VO01KPiOiSKRcfdbN0YTGwfTWj7XTKRZwLJ2zzW2BfgOqizfWB302207TUIyK6ZPo3+sX2cklHUqZOGQFOtX2VpOOAJbYXAn8PfF7SOynnlMNtr2xOrUckqUdE9KCf9yi1vYgyTLFz3TEdj6+mzHjbtST1iIgu2Wr33C9NoPumsc7PNqk1hoU3XlTr8cdd9lD9b7bjnjuv7hAAGP3dpGXHiNXW9NvZPeqTekTEsJT51Js990uSekRE15p/4+kk9YiILhn6NvplUJLUIyK6NH5FaZMlqUdE9KDO+492I0k9IqJLNo2/SUaSekRED1J+iYhoiX5OEzAoSeoREV0qE3qlpR4R0RKZJiAiolVyRWlEREtk9EtERMuk/BIR0RJGLE9Sj4hoh9aMfpH0fuAwyr3xbgQuAe4G5gPTgeuA19m+X9JpwAPAs4AtgDcCrwd2By62fXi1zxcBHwTWA34F/JXteyV9BDgQWA6ca/sf+vKTRkT0QdPLL1NGJ+k5wKuAXYD9gfG7Y3/T9nNs7wJcQ7nr9bjHUZL4Oyk3Uv0k8AxgJ0nPlDQDeB+wn+1nA0uAoyRtBrwCeIbtnYEPrSKm+ZKWSFoy+sB9Pf/QERGrxWVCr26WunTTUt8DONP2g8CDkr5Trd9R0oeAxwIbU26eOu47ti3pSuBW21cCSLoK2BaYCewA/EQSlNb+hZTW/4PAFySdBZy1soBsLwAWAGzwxFmT3oQ1IqJf2n6TjNOAl9u+XNLhwN4drz1UfR3reDz+fB1gFDjP9iETdyppLrAv8GrgSGCfNYgxIqKvml5T76Y49BPgZZLWl7Qx8NJq/SbALZLWBQ7t8bgXAXtIegqApI0kPbXa/2OqO2y/k1LyiYhoBAPLx6Z1tdRlypa67cWSFgJXALcCV1LKJO8HLqZ0nl5MSfJdsf27qnX/FUnrVavfB9wDnClpfUDAUd3/KBERg9Wmm2R83PaxkjYEfgRcYvtS4OSJG46Pbqke/xrYcRWv/QB4zkqONbfLmCIihq4tNfUFknYA1gdOrxJ6RMTaxc2vqXeV1G2/dtCBREQ0XWsuPoqIiGqagBo7QbuRpB4R0QOnpR4R0R5t6SiNiFjruS0dpRERUaT8EhHRGu25+CgiYq1nYDSjXyIiWsKlrt5kSeoRET3I6JcBW/e2+9n6U0tqjeHl/9qM2YG11RPqDoHDf3Je3SEAcPo+e9YdAsuX3lR3CNFnJh2lEREt0vyO0mZX/CMiGsbubumGpHmSrpV0naSjV7HNX0q6WtJVkv5jqn2mpR4R0SUbxvo0+kXSCHAS8EJgKbBY0kLbV3dssz3wHmAP23dK2mKq/aalHhHRgz7eeHoucJ3t620/DJwBHDRhmyOAk2zfCWD7tql2mqQeEdGDPpZftgZu7Hi+tFrX6anAUyX9RNJFkuZNtdOUXyIietDD6JcZkjqH5i2wvaDHw60DbA/sDcwEfiRpJ9t3TfYNERHRBaNekvrttudM8vpNwKyO5zOrdZ2WAhfbXgbcIOl/KEl+8ap2mvJLRES33Nea+mJge0mzJU0HDgYWTtjm25RWOpJmUMox10+20yT1iIheuMtlqt3Yy4EjgXOAa4Cv2b5K0nGSDqw2Owe4Q9LVwAXAu2zfMdl+U36JiOhBP68otb0IWDRh3TEdjw0cVS1dSVKPiOhBJvSKiGiJzP0SEdEmBpLUuyNpxPZo3XFEREzGY3VHMLmhjX6R9G1Jl1ST0syv1t0r6ROSLgd2l3SYpJ9J+rmkz1VzI0RENEQZp97NUpdhDml8o+1dgTnA2yRtBmxEGVi/C3AH8BrKxDXPBEaBQ4cYX0TE1Po0pHFQhll+eZukV1SPZ1GuihoFvlGt2xfYlTJTGcAGwEonr6la+vMB1mfDAYYcEdHB6SgFQNLewH7A7rbvl/RDYH3gwY46uoDTbb9nqv1V8ycsANh02mYNH2AUEa3S8IwzrPLLY4A7q4T+NGC3lWxzPvDq8fmCJT1e0pOGFF9ERHes7paaDCupfw9YR9I1wEeAiyZuUE0M/z7gXElXAOcBWw4pvoiI7qSmDrYfAvZfyUsbT9juq8BXhxFTRETPMk49IqJdMk1ARESbJKlHRLRIyi8RES1hUMOnCUhSj4joWr3DFbuRpB4R0YvU1CMiWiRJPSKiRZLUIyJaIhcfRUS0S9NHvwxzPvWIiBiwR39L3cbLl9Ubwr31Hn+cfvVA3SFw+ov3rjsEAPyl+v8m6xzyhLpDAGD5/95adwitotTUIyJaJDX1iIiWqHla3W4kqUdE9KDpHaVJ6hERvUhLPSKiRZLUIyLaQc7ol4iIdsnol4iIFklLPSKiPTL6JSKiLVJTj4homST1iIgWSVKPiGiPppdfhj71rqS3SbpG0r8P+9gREW1Xx3zqfwu80PahU20oKZ8kIqI5XEa/dLN0Q9I8SddKuk7S0ZNs9ypJljRnqn0ONWlK+ldgO+C7kk4D9qqe3w/Mt32FpGOBJ1frfwscMswYIyIm1afyi6QR4CTghcBSYLGkhbavnrDdJsDbgYu72e9QW+q23wzcDPw5sC1wme2dgfcCX+zYdAdgP9srTeiS5ktaImnJMh4acNQRER3c5TK1ucB1tq+3/TBwBnDQSrb7J+CjwIPd7LTO29ntCXwJwPYPgM0kbVq9ttD2Km/jY3uB7Tm256zLekMINSICxIr5X6ZagBnjjc9qmT9hd1sDN3Y8X1qtW3E86dnALNtndxtjU2vW99UdQETESnVffrnd9pQ18FWRNA04ATi8l++rs6X+X8ChAJL2pvwC/lBjPBERk+tvR+lNwKyO5zOrdeM2AXYEfijp18BuwMKpOkvrbKkfC5wq6QpKR+kbaowlIqI7/RunvhjYXtJsSjI/GHjtI4ex7wZmjD+X9EPgH2wvmWynQ0/qtrftePrylbx+7NCCiYjoUb8uPrK9XNKRwDnACHCq7askHQcssb1wdfbb1Jp6REQz9fGKUtuLgEUT1h2zim337mafSeoREd3qfrhibZLUIyJ60PS5X5LUIyJ6kJtkRES0SVrqEREtkZp6RER7qFqaLEk9IqIXaalHRLRHRr9ERLRJRr9ERLSE01KPIfLoaN0hMLr0lrpDAODBj+9Sdwi86vxz6w4BgM9e/fy6Q2D2/KV1h1D8vg/7SFKPiGiPtNQjItokST0ioiWcaQIiItolLfWIiHYYv/F0kyWpR0T0Ikk9IqI95GZn9ST1iIhuZZbGiIh2yeiXiIgWSUdpRESbJKlHRLTEo2BCr2mD2KmkbSX9YsK6OZI+vYrtfy1pxiBiiYjoK3e51GRoLXXbS4AlwzpeRES/CdBYs5vqA2mpd5K0naTLJL1L0lnVus0knSvpKkmnUN32T9JGks6WdLmkX0h6zaDji4johdzdUpeBJnVJfwZ8AzgcWNzx0geAH9t+BvAtYJtq/TzgZtu72N4R+N4g44uI6Em3pZeWJvXNgTOBQ21fPuG15wNfBrB9NnBntf5K4IWSPippL9t3r2zHkuZLWiJpyTIeGlD4ERF/SmPdLXUZZFK/G/gtsGe332D7f4BnU5L7hyQds4rtFtieY3vOuqzXl2AjIrrS8Jb6IDtKHwZeAZwj6V7g5o7XfgS8lpK49wceByBpK+D3tr8s6S7grwcYX0REz5o+pHGgo19s3yfppcB5wD91vPRB4CuSrgJ+SmnRA+wEfEzSGLAMeMsg44uI6ImbP/plIEnd9q+BHavHdwHPqV5aWK27A3jRSr71nGqJiGimZuf0XFEaEdGt3CQjIqJN7LI0WJJ6REQP0lKPiGiThif1gU8TEBHRGgaNuqulG5LmSbpW0nWSjl7J60dJulrSFZLOl/SkqfaZpB4R0Ys+XXwkaQQ4Cdgf2AE4RNIOEza7DJhje2fg68DxU+03ST0iogd9nNBrLnCd7ettPwycARzUuYHtC2zfXz29CJg51U6T1CMiejE+AmaqZWpbAzd2PF9arVuVNwHfnWqn6SiNiOhBD6NfZkjqvIfEAtsLVuuY0mHAHOAFU22bpB4R0SX1Nk3A7bbnTPL6TcCsjuczq3V/fExpP+AfgRfYnnJa2pRfIiJ6MdblMrXFwPaSZkuaDhxMNZXKOEnPAj4HHGj7tm522o6WesOv8FqbeNnDdYcAwAbnX1l3CCw6dI+6QwBAL9uk7hBYdNUFdYcAwMiWa74P9Snf2F4u6UjKfFcjwKm2r5J0HLDE9kLgY8DGwH9KAvit7QMn2287knpExDD0ea5024uARRPWHdPxeL9e95mkHhHRtcz9EhHRKpn7JSKiLappAposST0iohcpv0REtEizc3qSekREL/o1pHFQktQjInqRpB4R0RKm26tFa5OkHhHRJWE01uysnqQeEdGLlF8iIloi5ZeIiHbJ6JeIiDZJUo+IaAkb0lEaEdEizc7pj86kLmk+MB9gfTasOZqIWJukpj4A1c1bFwBsqsc3+zccEe2SpB4R0RIGur/xdC2S1CMiutb8Ox9NqzuAyUhaJGmruuOIiHjE2Fh3S00a3VK3fUDdMUREPCLll4iINjG42WMak9QjInrR8Jp6knpERLdSfomIaJm01CMi2iJzv0REtIdJUo+IaJWUXyIiWiRJPSKiLZzRLxERrWHw6GjdUUwqST0iohcpvwzWPdx5+/f99d+swS5mALf3K5410IQ4mhAD9COOBxoQw2VrHENT4ljjGEaOW+MY+hIH8KQ1+u7czm7wbG++Jt8vaYntOf2K59EcRxNiaEocTYihKXE0IYYmxZGWekREizgt9YiItmj+TTKS1Kt7nTZAE+JoQgzQjDiaEAM0I44mxABNiMNAw0e/yA0/60RENMWm0zbzbuu8uKttz1v2lUum6gOQNA84ERgBTrH9kQmvrwd8EdgVuAN4je1fT7bPRt/OLiKiUVzdJKObZQqSRoCTgP2BHYBDJO0wYbM3AXfafgrwSeCjU+03ST0iogcec1dLF+YC19m+3vbDwBnAQRO2OQg4vXr8dWBfSZpsp6mpN5AkOXWxiMa5hzvP+f7Y12Z0ufn6kpZ0PF9gu7NfYGvgxo7nS4HnTtjHI9vYXi7pbmAzJhmvv9YmdUnT7GbebHA8oUuaYbu2i4HGTy4Tvw7r2MBTgV/aHpO07VS1xDZq8vu0U2ecbW6U2J5XdwxTWSvLL5JGOt6AsyQ9tnpc++9j/KOVpLdQfRSb6uPWgOKY1vGPuTn80clmGPE8EdgHOF7SFcDTh3DMKUn6oKSXDOlY06oT2rqS1uxKyAHqiHMLSVu0NaEPwE3ArI7nM6t1K91G0jrAYygdpqtUexIbtiqhj0oakXQ28E/ABZKe2oQWUcc/xGyqP2Yd/yQdJ713A/8u6URJ84cVj+1bgA2AtwEX2/5uFU8t71kVRwOzbZ89jGNWifLPgB8Bu1X/1I1TxbkXcD5/WhOOVVsMbC9ptqTpwMHAwgnbLATeUD1+NfCDqf7/1pqkLukZkp5YJfTHAt8Dfmj7cMqcEl+StE0Ncanj8S6SFkh6D3AdcOvKthtQHBtJ2m7CujcBLwWOBH4O7CXpHQOMYeLP+FXgPcDtkl7V+QmrBq8DXgVcBcP5tCLpacB/AB+1/dWqprrhoI/bjWrkxvjj7SijON5s+/P1RfXoYns55X/rHOAa4Gu2r5J0nKQDq82+AGwm6TrgKODoqfbbyDP/gGwG/AB4gu27JH0Q+G9JPwA+QRlSdIakebb/MKygOkoaW1ES+XeBPSnDnJ5bnYB+AdwMXDLAUJ5PR6dNlbRmAl+0fa2kpcBvgNdL2rJqSffNhJrsvwB/AK60/QlJRwJ7Ab+VdAmwG/Cz6p9ioCQ9A9gF+DLwZODJknayfeUAjjViu/PKFlP+9sskvQ7Yu9rueNvX9vv43ZL0FGA6cLWkXYDHV3GOVJ9m9gbuB95i+9ZV7iiwvQhYNGHdMR2PHwT+opd9rjUtdds/Aj4g6fvV8x8DBwA32f4UcDzwHMq40KGS9Fzgn4HdbX/L9t9TztA/pbSQDwVeOeDSww7AmKRtJG1drbsUmF/VSe8DfgU8Friv3wfvqB2/FRg/wR0i6c3AZyl1xDdTTm7PHEZCr2xJaSE9Hxg/2bxM0qxJv6tH1UlttHq8T9X6HQMuBz5G+b3/mHLifVo/j91jnCPA9sDhks4H/tb2BcAy4C3AbcC7Kbll4kiOfhx/3X7vs3Vsr1ULcArwr9XjecB/UjrkTgL+EZheQ0xPonysOh54brXuacCZ1eNNhxTHPsDFlAscTqOc5N4PnE1pjb2VUraa0efjjlBafucDZ47/vMDu1br9O56/dEi/i8dXX9ej1Dp/CDyF0mH7L8B7gY369fN3/B6+S/lU8G3gXZSOsY2q17cCLgIOGPZ7tDr++BXo2wI3UBocO1fr1p3wfr4Y2LfPx59GGav9uDp+/kfLsta01DscAexStQDPB5YAx1LesP/schHAwEnaQNKXJT3J9m+A7wD3AAdK2pKS5LatNr+n+p6+1nGrDppp1ePtKSe1/YFbgJ0oH6E/QemBP5HSCXa4+zDMcsLPMlb93j9NKXXsXK2/BPgM8FFJc21faPusNT12F7G9FrhY0ia2H6L8bb5DOdn9FjiLctHIGn1imdDPMx14O3C+7cMof/vH2L67bKp9KCeWf3P5yD50ti1pfZehpUdQOvH2l/Q028sogR5BOSGdYPv8Ph9/DDjU9p393G/r1H1WqWMB1gd+DexXPe9ry3MVx9RK1n0aOA+YVj3fB/gJ8EFKC+1ZA4znAOBCVrS0dqJ8Wvmrav2zq/XbVF9HBhTHYcAJlNbwRpTy12Xjf5Nq3WuG9Ddar+PxKcA3Op4/D7gCOL2Px3s+cGvH8/mUk+i5wIc71s+kXH04d9C/g5XEONLx+FDKJ4mjgW0o1xF8htLZN0Ipt7wY+LNhx5llxbLWTuhVdUxeCmwHPOgBjaqoWqSd9dIXUTptfwzcDXyckkzeUL1+JnAl8HHbdw0ipo7YPkY5wX2EUrP+AbApsJNtS9oVOBz4kPvU4SWtuDCl6qx+EfA1Sp12BvB6ykntmZSyy1BGu0jajVLuuBG40PZXqyGvv7T9DkmHATsC/2m7bx3W1SfGV9veT2X8+0cpndPHV6+fQnmfnuwa/1kl7Qn8A/BNynDbOZQRQTtThtrtTamnv8Tl003UZK1N6gDVR8kHB3yMrWzfXD3+APBCyrjj51E6R39O+bh6A6VmOAocMai4tOJCkZnAccC+lBLUmykJ9mWUTw93AccAn7L9b30+tigJ/AOUE8b/StqC0uK7x/bHJF0MHG/7G/049hRxPYvSt3I0sAUlYV1D+eTyDUoZamdKLfuXAzj+F4C7bR8l6aRq9f8Ae1BGqL120O/TKeI7HPgw8G7bX5Y0g5Lgn0j5ZLcJ8Oe2z6wrxlhhbaypP2IICf1llBEBVGPgt7O9J6VFMx24zPbvKMn0SuBy26+z/WC/R7qomv2tSqqzKZ2fi4ADKR/vD6X0MSygtLoOBN7Wx4SujmMvpIy22ZmSyLF9G/BLVlxht8cwEnplc0qp5euUUUefoHQSb04p/fw9sIvtXw5oBNJfA3tIej3l93E+pWN6ie1XDjuhd/SzCMD2aZQT//7V89spZZfpwOds/yEJvUHqrv+0faGcOE+glDU+TfmH/QawQfX63OqrOr6nr/VryqiJz1PVjCmljrOADavnTweuBf6uM+4B/C52pCSHN3Y8X0gpP0C5evSLlJLQQGr4E+I5HngH8CHgeuDJ1fr1KC33Z03YfmAxsaKf54CJ74chvlc734M7Ulrnr6ueb0rpU3jn+LbV+2bPYceZZfJlrW6pD4NLTXgfSvK6G/hfSlJ7oLpi85PVCIhH5lXxH1+Askaq/d1s+wjgMElzKZ8U7gOeLmkD29dQhhK+R9LOHXH325aUcdcXSDqAcoXmPcDJVe34r4EP2H6wn7+DiSRtIulE4AmU1vjGwMPAxyQ9kTIkbxalJfqIQcbk0hp/HnBqddXoUOf7kXQQZeQRkvan9HNcDrxd0icpHaF/CbxF0mtcXONyvUc0yFpdUx+0CZ2Cl1BGcpwC7EcpNewJvMn2pUOI5eXAKynj3/cB/gZ4BqUE8wtK/fyntj8zwBhmUy773xv4EqVevA6lfnwRcIMHPKRU0vqUhLWp7b2rdTtQhujtRRnOuTXVpfmDjGVV8Xn45Zb5lBb406uLiz5O9cmO8onlF8Clto+T9EpKZ/arBnmSi9WXpD5gHZ2DT6SMu/4cpfQxh1LHvUMDnl5V0mOAUylln60oH5lfXv0zz6XUtv/N9smDiqEjlumUi0durUYgfRV4h/s4oqSLGA6m/D72tX1h1TL+MKWD+LtVfHdU27Z2Gtlx1aibA4HxMfk/pfSznEq5RH025ZPcR2x/vK44oztr09wvtagS+jSXER4HUxL6ma4my9efzvfRN9UohVHbd0o6nXIV5MHATpJOtP12YEHnCJ1Bs/2wpLsk7UvpYzhlmAm9iuGMavTPZyW90vYNknakmrsduGM8mbc9oVduA15CmQLhaNv3Vydc2b65ar1fTRkRFA2XlvqQdLTYXwf8HfACD3A8bzVE8FjK9LXvtX2LpA9Tyi2/pBrpYvvTg4phkthGKEMn77N93rCP3xHH5yl1/W8CD9h+a12xDFPHqJbx0uDTKf0Zolwp+9nqb3Q2ZYjtdsCxdZSjondJ6kMmaSdKCealtn8/4GPNolyluBflMv/XA//P9qeq+vZNg65hTxJb7WUNlTu1n0y5WvXAat26ri55byP98WyYe1PGmv/c9n+r3Nn+1ZQpqb+sMkPoX1SvL64t6OhJkvqQqdz0YLbt7w3xmK8HHke5YGSEMoxy6bCO32SSnkApiZ1t+9iawxkaSftRZr+8hDLy51u2v119knwBcJbtb9cZY6ye1NSHzGUe7KHOhW37iwCSlgHLk9BXqDps30QZh91aE1ro/0w1Csr2UpUJzF4t6VbKHe1nUTrU41EoLfWIltOKWzhOY8UEaZ8D5ti+rOo0fhllgrEPAEtt319fxLEmcvFRRMtVCX02ZfbHfwHWpVzZe3r1+lLgAuC/KXcGS0J/FEtLPaLlqg7PL1Kmq1iHUkt/LeUORRvbPqDabmPb99YWaPRFWuoR7bcesLT6ehzlBhZLKFfRzq1mDyUJvR3SURrRfrdTaumfp9w56L8kPQ74W8qUEbXdxDr6L0k9ouWqmvq3gHuBXSXdQpmD6BrbV9QbXfRbauoRa4Fq5MvuwDspM3RebvuEeqOKQUhSj1iLVFMErDfsmSBjeJLUIyJaJKNfIiJaJEk9IqJFktQjIlokST0iokWS1CMiWiRJPSKiRZLUIyJa5P8Dlm4UNCMJ2YkAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"}}]}]}